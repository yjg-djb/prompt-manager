# 场景：Flowise与Ollama本地化集成配置

## 核心用途

快速生成Flowise对接本地Ollama服务的完整配置流程，含参数设置、权限配置等。

---

## 提示词模板

### 集成需求

- **核心目标**：[描述具体集成目的]
  - 示例：实现Flowise通过Ollama调用`llama3:8b`模型完成文本生成
  - 示例：搭建基于Flowise+Ollama的本地RAG问答流程

- **环境约束**：[说明运行环境限制]
  - 示例：Windows10系统、Ollama v0.1.28、Flowise v1.6.0、无公网访问权限

- **额外要求**：[特殊配置需求]
  - 示例：支持流式输出、设置模型生成温度0.7、限制最大生成字符数2000

### 基础信息

- **Ollama服务状态**：[已启动/未启动]
- **Ollama服务地址**：[如 `localhost:11434`]
- **Flowise部署方式**：[Docker部署/本地源码部署]

### 期望输出要求

1. **前置准备检查清单**（Ollama服务配置、端口占用、依赖安装等）
2. **Flowise端集成步骤**（节点创建、参数填写、连接测试）
3. **关键参数说明**（各配置项的作用及推荐值）
4. **集成验证方法**（含测试用例、预期结果）
5. **常见问题预案**（如连接失败、生成超时的解决办法）

### 输出格式约束

- 步骤用有序列表呈现
- 关键参数用表格说明（参数名、作用、推荐值、填写位置）
- 代码/命令用 ``` 标记
- **重要注意事项**用加粗标注

---

## 使用示例

```
我需要在本地环境配置Flowise与Ollama的集成。

【集成需求】
- 核心目标：实现Flowise通过Ollama调用qwen:7b模型，完成企业文档问答
- 环境约束：Windows11、Ollama v0.1.30、Flowise v1.8.0、内网环境无公网
- 额外要求：支持流式输出、temperature=0.5、max_tokens=2000

【基础信息】
- Ollama服务状态：已启动
- Ollama服务地址：http://localhost:11434
- Flowise部署方式：Docker部署

请提供完整的集成配置指南，包含前置检查、配置步骤、参数说明和验证方法。
```
