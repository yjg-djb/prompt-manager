# 场景：开源AI应用离线部署方案设计

## 核心用途

生成开源AI应用（如RAG系统、问答机器人）的离线部署完整方案，确保无公网环境下正常运行。

**适用场景**：Flowise / Ollama 等本地AI应用

---

## 提示词模板

### 项目背景

- **应用名称/类型**：[说明要部署的应用]
  - 示例：基于Flowise+Ollama的RAG知识库系统
  - 示例：Ollama驱动的本地问答机器人
  - 示例：FastAPI+Ollama文本生成应用

- **部署环境**：[详细说明目标环境]
  - 示例：无公网企业内网、Windows Server 2019
  - 示例：CPU服务器（无GPU）、8核16GB内存、100GB存储

- **核心需求**：[说明部署后的功能需求]
  - 示例：离线部署后支持文档上传、问答交互
  - 示例：无外部依赖，所有组件本地运行
  - 示例：支持50人内部使用，响应时间<2s

### 技术栈信息

- **核心组件及版本**：[列出所有组件]
  - 示例：Flowise v1.6.0、Ollama v0.1.28、Chroma v0.4.20、FastAPI v0.104.1

- **依赖资源**：[说明需要的资源文件]
  - 示例：Ollama模型（llama3:8b）、Python依赖包、前端静态资源

- **现有条件**：[说明当前准备情况]
  - 示例：已获取所有开源组件源码
  - 示例：可通过内网传输文件
  - 示例：有基础Linux/Windows运维能力

### 期望输出要求

1. **离线部署前期准备**
   - 依赖资源收集
   - 环境预检查
   - 文件传输规划

2. **核心组件离线安装步骤**：
   - 基础环境配置（Python/Node.js/数据库等）
   - 各组件离线安装命令/操作（含依赖包离线安装）
   - 组件间连接配置（如Flowise对接本地Ollama、Chroma）

3. **离线资源部署**（模型文件、静态资源、配置文件）

4. **系统测试方案**（离线功能测试、性能测试、稳定性测试）

5. **部署验证步骤**（确认所有功能离线可用，无公网请求）

6. **运维保障方案**（离线环境下的故障排查、版本更新、数据备份）

7. **常见问题及应对措施**（安装失败、启动异常、功能不可用）

### 输出格式约束

- 步骤用有序列表呈现
- 命令用代码块标记
- 依赖资源清单用表格列出
- 配置文件内容用代码块展示
- 注意事项用高亮块标注

---

## 使用示例

```
我需要在企业内网环境离线部署一套AI问答系统。

【项目背景】
- 应用名称/类型：基于Flowise+Ollama的企业知识库问答系统
- 部署环境：
  - 企业内网，完全无公网访问
  - Windows Server 2022
  - 16核CPU、64GB内存、无GPU
  - 500GB SSD存储
- 核心需求：
  - 支持PDF/Word文档上传和解析
  - 自然语言问答，返回答案并引用来源
  - 支持100人内部使用，响应时间<3s

【技术栈信息】
- 核心组件及版本：Flowise v1.8.0、Ollama v0.3.0、Chroma v0.4.22
- 依赖资源：Ollama模型（qwen:7b，约4GB）、Python 3.11、Node.js 18
- 现有条件：
  - 可通过U盘传输文件
  - 已在联网环境测试通过
  - 有Windows服务器运维经验

请提供完整的离线部署方案，包含资源准备、安装步骤和验证测试。
```
