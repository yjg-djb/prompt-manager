# 场景：FastAPI封装Ollama模型接口开发

## 核心用途

生成FastAPI集成Ollama模型的完整接口代码，含参数配置、请求响应处理等。

---

## 提示词模板

### 接口需求

- **核心功能**：[描述接口要实现的功能]
  - 示例：封装Ollama-llama3:8b模型的文本生成接口
  - 示例：实现基于上下文的多轮对话接口
  - 示例：提供文本总结/翻译接口

- **请求参数**：[定义接口入参]
  - 示例：提问文本（prompt）、生成温度（temperature）、最大长度（max_tokens）
  - 示例：对话历史（history）、当前提问（query）

- **响应要求**：[定义返回格式]
  - 示例：返回生成文本、生成耗时、模型名称
  - 示例：支持流式响应（SSE）
  - 示例：响应格式为JSON

- **权限控制**：[说明鉴权需求]
  - 无需权限 / 需要API密钥验证 / 需要Token验证

### 技术约束

- **FastAPI版本**：[填写版本号]
  - 示例：0.104.1、0.110.0

- **Ollama版本**：[填写版本号]
  - 示例：v0.1.28、v0.3.0

- **运行环境**：[说明部署环境]
  - Windows10/macOS/Linux、是否部署在服务器

- **额外要求**：[其他技术要求]
  - 示例：接口需支持CORS跨域、添加接口文档注释、实现请求频率限制

### 期望输出要求

1. **完整接口代码**（含依赖导入、应用实例创建、接口定义、参数校验、Ollama调用逻辑、响应处理）
2. **依赖安装命令**（明确需要安装的包及版本）
3. **接口文档说明**（请求方式、参数列表、响应示例、错误码说明）
4. **启动命令及部署建议**
5. **接口测试用例**（含curl命令或Python请求代码）

### 输出格式约束

- 代码用Python代码块标记
- 依赖列表用表格呈现
- 接口说明分点清晰
- 测试用例含输入输出示例

---

## 使用示例

```
我需要开发一个FastAPI接口来封装Ollama模型服务。

【接口需求】
- 核心功能：封装Ollama-qwen:7b模型，提供文本生成和多轮对话接口
- 请求参数：
  - POST /api/generate: prompt(str), temperature(float, default=0.7), max_tokens(int, default=1000)
  - POST /api/chat: messages(list[dict]), temperature(float)
- 响应要求：JSON格式，包含生成文本、模型名称、生成耗时（ms）
- 权限控制：需要API Key验证（通过Header传递）

【技术约束】
- FastAPI版本：0.110.0
- Ollama版本：v0.3.0
- 运行环境：Linux服务器部署
- 额外要求：支持CORS跨域、完整的Swagger文档、请求频率限制（每分钟60次）

请提供完整的接口代码、部署指南和测试用例。
```
